# -*- coding: utf-8 -*-
"""RandomForestPRML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxao6yYoxJq1xkpH1AKZFuLABRTlySqN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.model_selection import train_test_split
np.random.seed(42)
df= pd.read_csv("Train_data.csv")
def le(df):
    for col in df.columns:
        if df[col].dtype == 'object':
                label_encoder = LabelEncoder()
                df[col] = label_encoder.fit_transform(df[col])

le(df)
x=  df.drop("class",axis= 1)
y= df["class"]
x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= 0.2, random_state= 42)

from collections import Counter

class MyRandomForest:
    def __init__(self, n_trees=10,sample_ratio=0.3, max_depth=None, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.sample_ratio = sample_ratio
        self.n_features = n_features
        self.trees = []

    def bootstrap_sample(self, X, y):
        n_samples = X.shape[0]
        indices = np.random.choice(n_samples, int(n_samples* self.sample_ratio), replace=True)
        return X.iloc[indices], y.iloc[indices]

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            X_sample, y_sample = self.bootstrap_sample(X, y)
            tree = DecisionTreeClassifier(
                max_depth=self.max_depth,
                max_features=self.n_features,
                # random_state= 42
            )
            tree.fit(X_sample, y_sample)
            # plt.figure(figsize=(30, 20))
            # plot_tree(tree)
            # plt.show()
            self.trees.append(tree)

    def predict(self, X):
        tree_predictions = np.array([tree.predict(X) for tree in self.trees])
        final_predictions = []

        for sample_preds in tree_predictions.T:
            # Majority vote
            vote = Counter(sample_preds).most_common(1)[0][0]
            final_predictions.append(vote)

        return np.array(final_predictions)

import joblib
# best_sample, best_trees, best_depth = grid_search_rf(x, y, sample_ratios, n_trees_options, max_depths)
best_sample=0.5
best_trees= 15
best_depth = None
final_model = MyRandomForest(n_trees=best_trees, sample_ratio=best_sample, max_depth=best_depth)
final_model.fit(x_train, y_train)


joblib.dump(final_model, 'random_forset.pkl')
# print(f"Final Model Accuracy: {final_acc:.4f}")

!pip install scapy
import csv
import os
from scapy.all import sniff, IP, TCP, UDP
from collections import defaultdict, deque
import time
import socket

# ======= File setup =======
csv_file_path = "/content/real_time_nids_features.csv"
csv_header = [
    "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes", "land",
    "wrong_fragment", "urgent", "hot", "num_failed_logins", "logged_in", "num_compromised",
    "root_shell", "su_attempted", "num_root", "num_file_creations", "num_shells",
    "num_access_files", "num_outbound_cmds", "is_host_login", "is_guest_login",
    "count", "srv_count", "serror_rate", "srv_serror_rate", "rerror_rate",
    "srv_rerror_rate", "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
    "dst_host_count", "dst_host_srv_count", "dst_host_same_srv_rate",
    "dst_host_diff_srv_rate", "dst_host_same_src_port_rate",
    "dst_host_srv_diff_host_rate", "dst_host_serror_rate",
    "dst_host_srv_serror_rate", "dst_host_rerror_rate",
    "dst_host_srv_rerror_rate"
]

# Write header only once
if not os.path.exists(csv_file_path):
    with open(csv_file_path, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(csv_header)

# ======= Flow tracking setup =======
flows = defaultdict(lambda: {
    'start_time': None,
    'end_time': None,
    'src_bytes': 0,
    'dst_bytes': 0,
    'count': 0,
    'services': set(),
    'dst_hosts': set(),
    'ports': set(),
    'tcp_flags': [],
    'logged_in': 0,
})

window = deque(maxlen=1000)
window_duration = 2  # seconds

def map_port_to_service(port):
    common_ports = {
        20: 'ftp_data',
        21: 'ftp',
        22: 'ssh',
        23: 'telnet',
        25: 'smtp',
        53: 'domain',
        69: 'tftp_u',
        80: 'http',
        110: 'pop_3',
        111: 'sunrpc',
        119: 'nntp',
        123: 'ntp_u',
        135: 'epmap',
        137: 'netbios_ns',
        138: 'netbios_dgm',
        139: 'netbios_ssn',
        143: 'imap4',
        161: 'snmp',
        162: 'snmptrap',
        179: 'bgp',
        443: 'https',
        445: 'microsoft_ds',
        513: 'login',
        514: 'shell',
        515: 'printer',
        520: 'route',
        540: 'uucp',
        635: 'mountd',
        8080: 'http',
    }

    if port in common_ports:
        return common_ports[port]
    elif port >= 1024:
        return 'private'
    else:
        return 'other'

def get_service(port):
    try:
        return socket.getservbyport(port)
    except OSError:
        return str(port)

def map_flags_to_dataset(flag):
    if flag in ['PA', 'A', 'P', 'FA', 'F']:
        return 'SF'
    elif flag == 'S':
        return 'S0'
    elif flag in ['R', 'RA']:
        return 'REJ'
    else:
        return 'OTH'

def extract_features(pkt):
    if not IP in pkt:
        return

    ip = pkt[IP]
    proto = pkt.proto
    now = time.time()

    if TCP in pkt or UDP in pkt:
        sport = pkt.sport
        dport = pkt.dport
    else:
        sport = dport = 0

    key = (ip.src, ip.dst, sport, dport, proto)

    flow = flows[key]
    if flow['start_time'] is None:
        flow['start_time'] = now
    flow['end_time'] = now

    flow['count'] += 1
    size = len(pkt)

    if ip.src == key[0]:
        flow['src_bytes'] += size
    else:
        flow['dst_bytes'] += size

    flow['services'].add(map_port_to_service(dport))
    flow['dst_hosts'].add(ip.dst)
    flow['ports'].add(dport)

    if TCP in pkt:
        tcp = pkt[TCP]
        flow['tcp_flags'].append(tcp.flags)
        if tcp.flags == "S":
            flow['logged_in'] = 0
        if tcp.flags == "PA":
            flow['logged_in'] = 1

    window.append({
        'src': ip.src,
        'dst': ip.dst,
        'proto': proto,
        'service': map_port_to_service(dport),
        'timestamp': now
    })

    duration = flow['end_time'] - flow['start_time']
    protocol_type = {6: 'tcp', 17: 'udp', 1: 'icmp'}.get(proto, 'other')
    service = list(flow['services'])[0] if flow['services'] else '0'
    raw_flag = flow['tcp_flags'][-1] if flow['tcp_flags'] else '0'
    flag = map_flags_to_dataset(str(raw_flag))  # Normalize it
    src_bytes = flow['src_bytes']
    dst_bytes = flow['dst_bytes']
    land = 1 if ip.src == ip.dst and sport == dport else 0

    wrong_fragment = 0
    urgent = 0
    hot = 0
    num_failed_logins = 0
    logged_in = flow['logged_in']
    num_compromised = 0
    root_shell = 0
    su_attempted = 0
    num_root = 0
    num_file_creations = 0
    num_shells = 0
    num_access_files = 0
    num_outbound_cmds = 0
    is_host_login = 0
    is_guest_login = 0

    recent = [w for w in window if now - w['timestamp'] <= window_duration]
    count = len([w for w in recent if w['dst'] == ip.dst])
    srv_count = len([w for w in recent if w['dst'] == ip.dst and w['service'] == service])

    serror_rate = 0
    srv_serror_rate = 0
    rerror_rate = 0
    srv_rerror_rate = 0
    same_srv_rate = srv_count / count if count else 0
    diff_srv_rate = 1 - same_srv_rate
    srv_diff_host_rate = 0

    dst_host_count = count
    dst_host_srv_count = srv_count
    dst_host_same_srv_rate = same_srv_rate
    dst_host_diff_srv_rate = diff_srv_rate
    dst_host_same_src_port_rate = 0
    dst_host_srv_diff_host_rate = 0
    dst_host_serror_rate = 0
    dst_host_srv_serror_rate = 0
    dst_host_rerror_rate = 0
    dst_host_srv_rerror_rate = 0

    features = [
        duration, protocol_type, service, flag, src_bytes, dst_bytes, land,
        wrong_fragment, urgent, hot, num_failed_logins, logged_in, num_compromised,
        root_shell, su_attempted, num_root, num_file_creations, num_shells,
        num_access_files, num_outbound_cmds, is_host_login, is_guest_login,
        count, srv_count, serror_rate, srv_serror_rate, rerror_rate,
        srv_rerror_rate, same_srv_rate, diff_srv_rate, srv_diff_host_rate,
        dst_host_count, dst_host_srv_count, dst_host_same_srv_rate,
        dst_host_diff_srv_rate, dst_host_same_src_port_rate,
        dst_host_srv_diff_host_rate, dst_host_serror_rate,
        dst_host_srv_serror_rate, dst_host_rerror_rate,
        dst_host_srv_rerror_rate
    ]

    # Save row to CSV
    with open(csv_file_path, mode='a', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(features)

    print("Captured:", features[:5], "...")  # Print a summary

def start_sniffing(iface="eth0"):
    print("[*] Capturing packets & writing to:", csv_file_path)
    sniff(iface=iface, prn=extract_features, store=0)

if __name__ == "__main__":
    start_sniffing("eth0")  # Or "wlan0" or your actual Colab interface

import pandas as pd
from collections import Counter
from sklearn.preprocessing import LabelEncoder

# Load unlabeled data
unlabeled_data = pd.read_csv("real_time_nids_features.csv")
for col in unlabeled_data.columns:
    if unlabeled_data[col].dtype == 'object':
        le = LabelEncoder()
        unlabeled_data[col] = le.fit_transform(unlabeled_data[col].astype(str))
# model.predict(unlabeled_data)
preds = model.predict(unlabeled_data)

# Count predictions
prediction_counts = Counter(preds)
print(" Prediction Count:", dict(prediction_counts))

# Final Say
if prediction_counts:
    final_class = prediction_counts.most_common(1)[0][0]
    if final_class == 1 or final_class == 'anomaly':
        print(" Final Say: Anomaly Detected")
    else:
        print("Final Say: Normal Behavior")
else:
    print(" Final Say: No predictions made. Check your data or model.")